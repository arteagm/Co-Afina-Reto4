{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sIc_vI83CW5"
   },
   "source": [
    "## OBTAINING ALL TWEETS FROM A USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zg5BuU9a3gy7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZY8WyH504C0H"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade tweepy  # needed v.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k4Tv2ESo2BIw"
   },
   "outputs": [],
   "source": [
    "# LA LLAVE\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAF02cQEAAAAAAw4Ko%2BlvaP%2FEKo4jetBjHN%2BGEpY%3D8ot7Zeg7DytI7xLRXuZO23cSWSLBIAHPMKrw9b8jebi9j7nWsI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubalv7VSMNkO"
   },
   "source": [
    "#Extracci√≥n de data a partir del @username.\n",
    "Basado en Tweepy v4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZV7UJqdJ5Eid"
   },
   "outputs": [],
   "source": [
    "input_user = 'mferna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "06b-kj461hfr"
   },
   "outputs": [],
   "source": [
    "def extract_tweet_data(input_user):\n",
    "    \"\"\"\n",
    "    Extracts data from a Twitter account.\n",
    "\n",
    "    Parameters:\n",
    "    input_user: str\n",
    "        Twitter account to be analysed (without the @).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "    \"\"\"\n",
    "    # Authorize access\n",
    "    client = tweepy.Client(BEARER_TOKEN)\n",
    "\n",
    "    # Get user ID from username\n",
    "    user_id = client.get_user(username=input_user).data.id\n",
    "\n",
    "    # Get Tweets timeline (check all pages, max 100 tweets per page, max 32 pages)\n",
    "    df_list = []\n",
    "    df_len0 = []\n",
    "    df_len = []\n",
    "    for response in tweepy.Paginator(client.get_users_tweets, user_id,\n",
    "                                    tweet_fields='created_at,lang,public_metrics',\n",
    "                                    max_results=100, limit=32):\n",
    "        page_df = pd.DataFrame.from_dict(response.data)\n",
    "        df_len0.append(len(page_df))\n",
    "        # Delete non-spanish tweets\n",
    "        page_df = page_df.drop(page_df[page_df.lang !='es'].index)\n",
    "        page_df = page_df.drop(['lang'], axis=1)\n",
    "        df_len.append(len(page_df))\n",
    "        # Organize columns\n",
    "        retweet_count = []\n",
    "        like_count = []\n",
    "        for item in page_df['public_metrics'].tolist():\n",
    "          retweet_count.append(item['retweet_count'])\n",
    "          like_count.append(item['like_count'])\n",
    "        page_df = page_df.drop(['public_metrics'], axis=1)\n",
    "        page_df['retweet_count'] = retweet_count\n",
    "        page_df['like_count'] = like_count\n",
    "        df_list.append(page_df)\n",
    "    es_tweets = round(sum(df_len)*100/sum(df_len0),1)\n",
    "    print(f'Obtained {es_tweets}% tweets in spanish from total of {sum(df_len0)} tweets from user @{input_user}.')\n",
    "    tweets_df = pd.concat(df_list, ignore_index=True)\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z30Yc-sFKSIz"
   },
   "source": [
    "OLD EXTRACTION CODE (only works for obtaining user info, and only 100 tweets max)\\\n",
    "Es una modificaci√≥n de los ejemplos para obtener [info del user](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Lookup/get_users_with_bearer_token.py) y [sus tweets](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Tweet-Timeline/user_tweets.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ySCOAv1-s5Je"
   },
   "outputs": [],
   "source": [
    "def create_url(username, twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      usernames = f'usernames={input_user}'\n",
    "      user_fields = 'user.fields=description,created_at,protected,public_metrics,verified'\n",
    "      url = f'https://api.twitter.com/2/users/by?{usernames}&{user_fields}'\n",
    "    elif twitter_object == 'tweet':\n",
    "      user_dict = get_json('user')\n",
    "      user_id = user_dict['data'][0]['id']\n",
    "      url = f'https://api.twitter.com/2/users/{user_id}/tweets?max_results=100'\n",
    "    else:\n",
    "      print('Error. Wrong Twitter Object.')\n",
    "      return None\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_params(twitter_object):\n",
    "    if twitter_object == 'tweet':\n",
    "      params = {\"tweet.fields\": \"created_at,lang,public_metrics\"}\n",
    "    else:\n",
    "      print('Error. Wrong Twitter Object.')\n",
    "      return None\n",
    "    return params\n",
    "\n",
    "\n",
    "def bearer_oauth_user(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def bearer_oauth_tweet(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      response = requests.request(\"GET\", url, auth=bearer_oauth_user,)\n",
    "    elif twitter_object == 'tweet':\n",
    "      params = get_params('tweet')\n",
    "      response = requests.request(\"GET\", url, auth=bearer_oauth_tweet,params=params)\n",
    "    else:\n",
    "        print('Error. Wrong Twitter Object.')\n",
    "        return None\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request returned an error: {response.status_code} {response.text}')\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_json(twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      url = create_url(input_user, 'user')\n",
    "      json_response = connect_to_endpoint(url, 'user')\n",
    "    elif twitter_object == 'tweet':\n",
    "      url = create_url(input_user, 'tweet')\n",
    "      json_response = connect_to_endpoint(url, 'tweet')\n",
    "    else:\n",
    "        print('Error. Wrong Twitter Object.')\n",
    "        return None\n",
    "    return json_response\n",
    "\n",
    "\n",
    "def extract_user_data(username):  # MAIN FILE\n",
    "    \"\"\"\n",
    "    Extracts data from a Twitter account.\n",
    "\n",
    "    Parameters:\n",
    "    input_user: str\n",
    "        Twitter account to be analysed (without the @).\n",
    "    Returns:\n",
    "    dict\n",
    "        A dict containing the releveant variables about the user\n",
    "    \"\"\"\n",
    "    # Get user ID from username\n",
    "    user_dict = get_json('user')\n",
    "    #user_id = user_dict['data'][0]['id']\n",
    "\n",
    "    # Get Tweets timeline\n",
    "    #tweet_dict = get_json('tweet')\n",
    "    #tweet_pd = pd.DataFrame.from_dict(tweet_dict['data'])\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzybELSiTdWI"
   },
   "source": [
    "#OUTPUTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUt6St5mOAxL",
    "outputId": "41cb761d-1fa1-4fe2-e2a2-7669cd2c8fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'verified': False,\n",
       "   'created_at': '2008-04-02T01:08:53.000Z',\n",
       "   'description': 'M√©dico-Epidem√≠ologo, Doctor en Educaci√≥n, Profesor Titular,Coordinador de Investigaci√≤n Facultad Med UCV. hablamos de TIC en Investigaci√≥n, Educaci√≥n y Salud.',\n",
       "   'public_metrics': {'followers_count': 6093,\n",
       "    'following_count': 2968,\n",
       "    'tweet_count': 58147,\n",
       "    'listed_count': 300},\n",
       "   'username': 'mferna',\n",
       "   'id': '14279589',\n",
       "   'name': 'Mariano Fern√°ndez S',\n",
       "   'protected': False}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_user_data(input_user)  ## Dict with user info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "ffWCJ56kOoLk",
    "outputId": "a8a54848-f0a4-4b93-f2b1-402953beaafb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 73.3% tweets in spanish from total of 3198 tweets from user @mferna.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-03 01:37:08+00:00</td>\n",
       "      <td>1521302809450369026</td>\n",
       "      <td>Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-27 15:56:31+00:00</td>\n",
       "      <td>1519344752981909504</td>\n",
       "      <td>RT @gorka_orive: Os recomiendo seguir la infor...</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27 15:56:09+00:00</td>\n",
       "      <td>1519344657590800391</td>\n",
       "      <td>RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-27 14:56:02+00:00</td>\n",
       "      <td>1519329529474555905</td>\n",
       "      <td>Sobre la Hepatitis Infantil Desconocida... htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-26 01:20:00+00:00</td>\n",
       "      <td>1518761781325451266</td>\n",
       "      <td>Adenovirus 41 o covid, sospechosos tras las he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>2019-10-02 14:51:45+00:00</td>\n",
       "      <td>1179408633756618754</td>\n",
       "      <td>RT @campussanofi: üëçüëè @AbbottGlobal y @sanofi s...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>2019-09-30 23:32:52+00:00</td>\n",
       "      <td>1178815001123065861</td>\n",
       "      <td>RT @opsoms: CONVOCATORIA DE ART√çCULOS ‚ùóÔ∏è\\n\\nüìò ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>2019-09-30 00:23:19+00:00</td>\n",
       "      <td>1178465311672143874</td>\n",
       "      <td>RT @biotecfarffucv: En #Biotecfar #SalvamosVid...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>2019-09-30 00:19:54+00:00</td>\n",
       "      <td>1178464452481605638</td>\n",
       "      <td>Cu√°ntas tazas de caf√© hay que tomar al d√≠a par...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>2019-09-30 00:19:18+00:00</td>\n",
       "      <td>1178464301641846784</td>\n",
       "      <td>Los vegetarianos tienen menor riesgo de infart...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2344 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                   id  \\\n",
       "0    2022-05-03 01:37:08+00:00  1521302809450369026   \n",
       "1    2022-04-27 15:56:31+00:00  1519344752981909504   \n",
       "2    2022-04-27 15:56:09+00:00  1519344657590800391   \n",
       "3    2022-04-27 14:56:02+00:00  1519329529474555905   \n",
       "4    2022-04-26 01:20:00+00:00  1518761781325451266   \n",
       "...                        ...                  ...   \n",
       "2339 2019-10-02 14:51:45+00:00  1179408633756618754   \n",
       "2340 2019-09-30 23:32:52+00:00  1178815001123065861   \n",
       "2341 2019-09-30 00:23:19+00:00  1178465311672143874   \n",
       "2342 2019-09-30 00:19:54+00:00  1178464452481605638   \n",
       "2343 2019-09-30 00:19:18+00:00  1178464301641846784   \n",
       "\n",
       "                                                   text  retweet_count  \\\n",
       "0     Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...              2   \n",
       "1     RT @gorka_orive: Os recomiendo seguir la infor...            115   \n",
       "2     RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...             74   \n",
       "3     Sobre la Hepatitis Infantil Desconocida... htt...              0   \n",
       "4     Adenovirus 41 o covid, sospechosos tras las he...              1   \n",
       "...                                                 ...            ...   \n",
       "2339  RT @campussanofi: üëçüëè @AbbottGlobal y @sanofi s...              9   \n",
       "2340  RT @opsoms: CONVOCATORIA DE ART√çCULOS ‚ùóÔ∏è\\n\\nüìò ...             13   \n",
       "2341  RT @biotecfarffucv: En #Biotecfar #SalvamosVid...             81   \n",
       "2342  Cu√°ntas tazas de caf√© hay que tomar al d√≠a par...              0   \n",
       "2343  Los vegetarianos tienen menor riesgo de infart...              0   \n",
       "\n",
       "      like_count  \n",
       "0              1  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "2339           0  \n",
       "2340           0  \n",
       "2341           0  \n",
       "2342           0  \n",
       "2343           0  \n",
       "\n",
       "[2344 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tweet_data(input_user)  ## DataFrame with all tweets in spanish from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaYmSBmS5Wb0"
   },
   "source": [
    "#Procesamiento de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C1yYEvhi8wsk"
   },
   "outputs": [],
   "source": [
    "def vars_word(word, input_df):\n",
    "    \"\"\"\n",
    "    Compute the variables for a given word.\n",
    "\n",
    "    Parameters:\n",
    "    word: str\n",
    "        Word to be analysed in the timeline tweets.\n",
    "    input_df: pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "\n",
    "    Returns:\n",
    "    tuple(float, float, float, float, float):\n",
    "        A tuple containing variables of the word: frequency, like rate, retweet\n",
    "        rate, popularity, and polemicity\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing counters of the word: number of times it appears\n",
    "        on all the tweets, number of tweets it appears in, number of retweets it\n",
    "        appears in.\n",
    "    \"\"\"\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Flatten text column\n",
    "    splitted_tweets=df.clean_text.apply(lambda a: a.split()).tolist()#.count(\"datos\")\n",
    "    # Flatten text column\n",
    "    total_word_list = [word_tweet for word_list in splitted_tweets for word_tweet in word_list]\n",
    "    \n",
    "\n",
    "    word_count = total_word_list.count(word)\n",
    "    total_word_count = len(total_word_list)\n",
    "    freq = word_count/total_word_count\n",
    "\n",
    "    df['word_count'] = df.clean_text.map(lambda t: t.count(word))\n",
    "    tweets_containing = df['word_count'].astype(bool).sum()\n",
    "    #if tweets_containing == 0:\n",
    "    #    return None\n",
    "    df.drop(['word_count'], axis=1)\n",
    "    # like_count_flag\n",
    "    like_count = df[df['word_count'].astype(bool)]['like_count'].sum()\n",
    "    retweet_count = df[df['word_count'].astype(bool)]['retweet_count'].sum()\n",
    "    #if (like_count == 0) or (retweet_count == 0):\n",
    "    #    return None\n",
    "    like_rate = like_count/tweets_containing\n",
    "    retweet_rate = retweet_count/tweets_containing\n",
    "\n",
    "    popularity  = retweet_rate/freq\n",
    "    polemicity  = retweet_rate/like_rate\n",
    "\n",
    "    variables = (freq, like_rate, retweet_rate, popularity, polemicity)\n",
    "    counters = (word_count, like_count, retweet_count)\n",
    "    return variables, counters\n",
    "\n",
    "\n",
    "def vars_cat(category, df):\n",
    "    \"\"\"\n",
    "    Compute the variables for a given category.\n",
    "\n",
    "    Parameters:\n",
    "    category: list\n",
    "        List of words to be analysed in group in the timeline tweets.\n",
    "    df: pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "\n",
    "    Returns:\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing counters of the words in the category: number of\n",
    "        times they appear on all the tweets, number of tweets they appears in,\n",
    "        number of retweets they appear in.\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing popularity and polemicity of the category.\n",
    "    \"\"\"\n",
    "\n",
    "    category = [word for word in category if word]\n",
    "    pop_cat = pol_cat = word_count_cat = like_count_cat = retweet_count_cat = 0\n",
    "    for word in category:\n",
    "        #if vars_word(word, df) is None:\n",
    "        vars, counters = vars_word(word, df)\n",
    "        pop_cat += counters[0] * vars[3]\n",
    "        pol_cat += counters[0] * vars[4]\n",
    "        word_count_cat += counters[0]\n",
    "        like_count_cat += counters[1]\n",
    "        retweet_count_cat += counters[2]\n",
    "\n",
    "    pop_cat = pop_cat / word_count_cat\n",
    "    pol_cat = pol_cat / word_count_cat\n",
    "\n",
    "    cat_counters = (word_count_cat, like_count_cat, retweet_count_cat)\n",
    "    pop_pol_cats = (pop_cat, pol_cat)\n",
    "    return cat_counters, pop_pol_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWNLObbkO5e3"
   },
   "source": [
    "#OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGZeprWKIO0m"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "    try:\n",
    "\n",
    "        final_string = \"\"\n",
    "\n",
    "        # minusculas\n",
    "        text = text.lower()\n",
    "        text = text.replace(\" u \",\" \").replace(\"√°\",\"a\").replace(\"√©\",\"e\").replace(\"√≠\",\"i\").replace(\"√≥\",\"o\").replace(\"√∫\",\"u\")\n",
    "\n",
    "        # quitar salto de lineas\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "        text=emoji_pattern.sub(r'', text)\n",
    "        # quitar signos gramaticales\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "\n",
    "        # stopwords\n",
    "        text = text.split()\n",
    "        useless_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "        useless_words = useless_words + ['...','considero','tal','vez','seria','debe','tener','siento', \"dar\", \"hacer\"\n",
    "                            'estan','tan','parece','ademas','debido','cuenta','hace','cada','toda','si', \"ser\",\n",
    "                                         \"ma\", \"mas\", \"m√°s\",\"bien\", \"buena\", \"creo\", \"aun\"]\n",
    "        # \n",
    "        text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "        # quitar numeros\n",
    "        text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "        \n",
    "\n",
    "        # Lematizacion\n",
    "        #if stem == 'Stem':\n",
    "        #    stemmer = PorterStemmer() \n",
    "        #    text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "        #elif stem == 'Lem':\n",
    "        #    lem = WordNetLemmatizer()\n",
    "        #    text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "        #elif stem == 'Spacy':\n",
    "        #    text_filtered = nlp(' '.join(text_filtered))\n",
    "        #    text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "        #else:\n",
    "        #    text_stemmed = text_filtered\n",
    "\n",
    "        #final_string = ' '.join(text_stemmed)\n",
    "        final_string = ' '.join(text_filtered)\n",
    "\n",
    "        # sinonimia\n",
    "        #final_string = final_string.replace('docent','profesor')\n",
    "    except AttributeError:\n",
    "        final_string = \"\"\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_wrapper_1(sent):\n",
    "    return list(nltk.ngrams(sent, 1))\n",
    "\n",
    "def ngrams_wrapper_2(sent):\n",
    "    return list(nltk.ngrams(sent, 2))\n",
    "\n",
    "def ngrams_wrapper_3(sent):\n",
    "    return list(nltk.ngrams(sent, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_bigrams_1(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_1, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram\n",
    "\n",
    "def make_list_bigrams_2(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_2, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram\n",
    "\n",
    "def make_list_bigrams_3(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_3, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_process_1(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_1(clean_string(str(text), stem='Stem'))])\n",
    "\n",
    "def nlp_process_2(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_2(clean_string(str(text), stem='Stem'))])\n",
    "\n",
    "def nlp_process_3(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_3(clean_string(str(text), stem='Stem'))])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fc216",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 73.3% tweets in spanish from total of 3198 tweets from user @mferna.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-03 01:37:08+00:00</td>\n",
       "      <td>1521302809450369026</td>\n",
       "      <td>Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-27 15:56:31+00:00</td>\n",
       "      <td>1519344752981909504</td>\n",
       "      <td>RT @gorka_orive: Os recomiendo seguir la infor...</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27 15:56:09+00:00</td>\n",
       "      <td>1519344657590800391</td>\n",
       "      <td>RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-27 14:56:02+00:00</td>\n",
       "      <td>1519329529474555905</td>\n",
       "      <td>Sobre la Hepatitis Infantil Desconocida... htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-26 01:20:00+00:00</td>\n",
       "      <td>1518761781325451266</td>\n",
       "      <td>Adenovirus 41 o covid, sospechosos tras las he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>2019-10-02 14:51:45+00:00</td>\n",
       "      <td>1179408633756618754</td>\n",
       "      <td>RT @campussanofi: üëçüëè @AbbottGlobal y @sanofi s...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>2019-09-30 23:32:52+00:00</td>\n",
       "      <td>1178815001123065861</td>\n",
       "      <td>RT @opsoms: CONVOCATORIA DE ART√çCULOS ‚ùóÔ∏è\\n\\nüìò ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>2019-09-30 00:23:19+00:00</td>\n",
       "      <td>1178465311672143874</td>\n",
       "      <td>RT @biotecfarffucv: En #Biotecfar #SalvamosVid...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>2019-09-30 00:19:54+00:00</td>\n",
       "      <td>1178464452481605638</td>\n",
       "      <td>Cu√°ntas tazas de caf√© hay que tomar al d√≠a par...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>2019-09-30 00:19:18+00:00</td>\n",
       "      <td>1178464301641846784</td>\n",
       "      <td>Los vegetarianos tienen menor riesgo de infart...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2344 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                   id  \\\n",
       "0    2022-05-03 01:37:08+00:00  1521302809450369026   \n",
       "1    2022-04-27 15:56:31+00:00  1519344752981909504   \n",
       "2    2022-04-27 15:56:09+00:00  1519344657590800391   \n",
       "3    2022-04-27 14:56:02+00:00  1519329529474555905   \n",
       "4    2022-04-26 01:20:00+00:00  1518761781325451266   \n",
       "...                        ...                  ...   \n",
       "2339 2019-10-02 14:51:45+00:00  1179408633756618754   \n",
       "2340 2019-09-30 23:32:52+00:00  1178815001123065861   \n",
       "2341 2019-09-30 00:23:19+00:00  1178465311672143874   \n",
       "2342 2019-09-30 00:19:54+00:00  1178464452481605638   \n",
       "2343 2019-09-30 00:19:18+00:00  1178464301641846784   \n",
       "\n",
       "                                                   text  retweet_count  \\\n",
       "0     Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...              2   \n",
       "1     RT @gorka_orive: Os recomiendo seguir la infor...            115   \n",
       "2     RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...             74   \n",
       "3     Sobre la Hepatitis Infantil Desconocida... htt...              0   \n",
       "4     Adenovirus 41 o covid, sospechosos tras las he...              1   \n",
       "...                                                 ...            ...   \n",
       "2339  RT @campussanofi: üëçüëè @AbbottGlobal y @sanofi s...              9   \n",
       "2340  RT @opsoms: CONVOCATORIA DE ART√çCULOS ‚ùóÔ∏è\\n\\nüìò ...             13   \n",
       "2341  RT @biotecfarffucv: En #Biotecfar #SalvamosVid...             81   \n",
       "2342  Cu√°ntas tazas de caf√© hay que tomar al d√≠a par...              0   \n",
       "2343  Los vegetarianos tienen menor riesgo de infart...              0   \n",
       "\n",
       "      like_count  \n",
       "0              1  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "2339           0  \n",
       "2340           0  \n",
       "2341           0  \n",
       "2342           0  \n",
       "2343           0  \n",
       "\n",
       "[2344 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy input (after cleaning)\n",
    "o_df = extract_tweet_data(input_user)\n",
    "#test_df = o_df.copy()\n",
    "o_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>bigram_key_words</th>\n",
       "      <th>trigram_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>1521302809450369026</td>\n",
       "      <td>Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>guia evaluacion conservacion datos investigaci...</td>\n",
       "      <td>('guia', 'evaluacion')/('evaluacion', 'conserv...</td>\n",
       "      <td>('guia', 'evaluacion', 'conservacion')/('evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1519344752981909504</td>\n",
       "      <td>RT @gorka_orive: Os recomiendo seguir la infor...</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>rt gorkaorive recomiendo seguir informacion va...</td>\n",
       "      <td>('rt', 'gorkaorive')/('gorkaorive', 'recomiend...</td>\n",
       "      <td>('rt', 'gorkaorive', 'recomiendo')/('gorkaoriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1519344657590800391</td>\n",
       "      <td>RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>rt drpontecarlosi nuevas guias uso aspirina pr...</td>\n",
       "      <td>('rt', 'drpontecarlosi')/('drpontecarlosi', 'n...</td>\n",
       "      <td>('rt', 'drpontecarlosi', 'nuevas')/('drponteca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1519329529474555905</td>\n",
       "      <td>Sobre la Hepatitis Infantil Desconocida... htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hepatitis infantil desconocida</td>\n",
       "      <td>('hepatitis', 'infantil')/('infantil', 'descon...</td>\n",
       "      <td>('hepatitis', 'infantil', 'desconocida')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>1518761781325451266</td>\n",
       "      <td>Adenovirus 41 o covid, sospechosos tras las he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>adenovirus  covid sospechosos tras hepatitis i...</td>\n",
       "      <td>('adenovirus', 'covid')/('covid', 'sospechosos...</td>\n",
       "      <td>('adenovirus', 'covid', 'sospechosos')/('covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>1518759259307225089</td>\n",
       "      <td>Hepatitis raras https://t.co/ooCqlbenPp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hepatitis raras httpstcooocqlbenpp</td>\n",
       "      <td>('hepatitis', 'raras')/('raras', 'httpstcooocq...</td>\n",
       "      <td>('hepatitis', 'raras', 'httpstcooocqlbenpp')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>1518231256831610881</td>\n",
       "      <td>Nuevas profesiones: la transformaci√≥n de la re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nuevas profesiones transformacion revolucion t...</td>\n",
       "      <td>('nuevas', 'profesiones')/('profesiones', 'tra...</td>\n",
       "      <td>('nuevas', 'profesiones', 'transformacion')/('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>1518231226548731906</td>\n",
       "      <td>El futuro laboral del colectivo universitario ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>futuro laboral colectivo universitario ‚Äì  http...</td>\n",
       "      <td>('futuro', 'laboral')/('laboral', 'colectivo')...</td>\n",
       "      <td>('futuro', 'laboral', 'colectivo')/('laboral',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>1518231170747711488</td>\n",
       "      <td>Presente y futuro de la realidad virtual | Enl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>presente futuro realidad virtual enlaces</td>\n",
       "      <td>('presente', 'futuro')/('futuro', 'realidad')/...</td>\n",
       "      <td>('presente', 'futuro', 'realidad')/('futuro', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>1518230665883435009</td>\n",
       "      <td>El juego de la escalera de las emociones ¬°Cont...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>juego escalera emociones ¬°controla emociones h...</td>\n",
       "      <td>('juego', 'escalera')/('escalera', 'emociones'...</td>\n",
       "      <td>('juego', 'escalera', 'emociones')/('escalera'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_at                   id  \\\n",
       "0  2022-05-03  1521302809450369026   \n",
       "1  2022-04-27  1519344752981909504   \n",
       "2  2022-04-27  1519344657590800391   \n",
       "3  2022-04-27  1519329529474555905   \n",
       "4  2022-04-26  1518761781325451266   \n",
       "5  2022-04-26  1518759259307225089   \n",
       "6  2022-04-24  1518231256831610881   \n",
       "7  2022-04-24  1518231226548731906   \n",
       "8  2022-04-24  1518231170747711488   \n",
       "9  2022-04-24  1518230665883435009   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...              2   \n",
       "1  RT @gorka_orive: Os recomiendo seguir la infor...            115   \n",
       "2  RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...             74   \n",
       "3  Sobre la Hepatitis Infantil Desconocida... htt...              0   \n",
       "4  Adenovirus 41 o covid, sospechosos tras las he...              1   \n",
       "5            Hepatitis raras https://t.co/ooCqlbenPp              0   \n",
       "6  Nuevas profesiones: la transformaci√≥n de la re...              0   \n",
       "7  El futuro laboral del colectivo universitario ...              0   \n",
       "8  Presente y futuro de la realidad virtual | Enl...              0   \n",
       "9  El juego de la escalera de las emociones ¬°Cont...              0   \n",
       "\n",
       "   like_count                                         clean_text  \\\n",
       "0           1  guia evaluacion conservacion datos investigaci...   \n",
       "1           0  rt gorkaorive recomiendo seguir informacion va...   \n",
       "2           0  rt drpontecarlosi nuevas guias uso aspirina pr...   \n",
       "3           0                    hepatitis infantil desconocida    \n",
       "4           0  adenovirus  covid sospechosos tras hepatitis i...   \n",
       "5           0                 hepatitis raras httpstcooocqlbenpp   \n",
       "6           0  nuevas profesiones transformacion revolucion t...   \n",
       "7           0  futuro laboral colectivo universitario ‚Äì  http...   \n",
       "8           0         presente futuro realidad virtual enlaces     \n",
       "9           0  juego escalera emociones ¬°controla emociones h...   \n",
       "\n",
       "                                    bigram_key_words  \\\n",
       "0  ('guia', 'evaluacion')/('evaluacion', 'conserv...   \n",
       "1  ('rt', 'gorkaorive')/('gorkaorive', 'recomiend...   \n",
       "2  ('rt', 'drpontecarlosi')/('drpontecarlosi', 'n...   \n",
       "3  ('hepatitis', 'infantil')/('infantil', 'descon...   \n",
       "4  ('adenovirus', 'covid')/('covid', 'sospechosos...   \n",
       "5  ('hepatitis', 'raras')/('raras', 'httpstcooocq...   \n",
       "6  ('nuevas', 'profesiones')/('profesiones', 'tra...   \n",
       "7  ('futuro', 'laboral')/('laboral', 'colectivo')...   \n",
       "8  ('presente', 'futuro')/('futuro', 'realidad')/...   \n",
       "9  ('juego', 'escalera')/('escalera', 'emociones'...   \n",
       "\n",
       "                                   trigram_key_words  \n",
       "0  ('guia', 'evaluacion', 'conservacion')/('evalu...  \n",
       "1  ('rt', 'gorkaorive', 'recomiendo')/('gorkaoriv...  \n",
       "2  ('rt', 'drpontecarlosi', 'nuevas')/('drponteca...  \n",
       "3           ('hepatitis', 'infantil', 'desconocida')  \n",
       "4  ('adenovirus', 'covid', 'sospechosos')/('covid...  \n",
       "5       ('hepatitis', 'raras', 'httpstcooocqlbenpp')  \n",
       "6  ('nuevas', 'profesiones', 'transformacion')/('...  \n",
       "7  ('futuro', 'laboral', 'colectivo')/('laboral',...  \n",
       "8  ('presente', 'futuro', 'realidad')/('futuro', ...  \n",
       "9  ('juego', 'escalera', 'emociones')/('escalera'...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_df[\"created_at\"]=o_df[\"created_at\"].apply(lambda a: pd.to_datetime(a).date())\n",
    "o_df[\"clean_text\"]=o_df[\"text\"].apply(lambda x: clean_string(x))\n",
    "o_df[\"bigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_2(x))\n",
    "o_df[\"trigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_3(x))\n",
    "\n",
    "o_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddbccd12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('rt', 'doctormacias')</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('rt', 'gorkaorive')</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('educacion', 'distancia')</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('rt', 'saberucv')</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('miradorsalud', 've')</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('iberoamericana', 'educacion')</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('revista', 'iberoamericana')</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('ried', 'revista')</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('rt', 'sandralopezleon')</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('antilopino', '‚Äú')</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('rt', 'antilopino')</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('ciencia', 'abierta')</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('rt', 'fuedicho')</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('rt', 'juliocastrom')</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('america', 'latina')</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('inteligencia', 'artificial')</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('universidad', 'central')</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('acceso', 'abierto')</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('pais', 'retina')</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('rt', 'rigotordoc')</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            bigrams   0\n",
       "0            ('rt', 'doctormacias')  56\n",
       "1              ('rt', 'gorkaorive')  49\n",
       "2        ('educacion', 'distancia')  37\n",
       "3                ('rt', 'saberucv')  30\n",
       "4            ('miradorsalud', 've')  25\n",
       "5   ('iberoamericana', 'educacion')  24\n",
       "6     ('revista', 'iberoamericana')  23\n",
       "7               ('ried', 'revista')  23\n",
       "8         ('rt', 'sandralopezleon')  22\n",
       "9               ('antilopino', '‚Äú')  20\n",
       "10             ('rt', 'antilopino')  20\n",
       "11           ('ciencia', 'abierta')  19\n",
       "12               ('rt', 'fuedicho')  18\n",
       "13           ('rt', 'juliocastrom')  17\n",
       "14            ('america', 'latina')  17\n",
       "15   ('inteligencia', 'artificial')  16\n",
       "16       ('universidad', 'central')  15\n",
       "17            ('acceso', 'abierto')  15\n",
       "18               ('pais', 'retina')  14\n",
       "19             ('rt', 'rigotordoc')  13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_word_bigram = o_df[o_df['clean_text']!='']['bigram_key_words'].to_list()\n",
    "all_key_word_bigram = '/'.join([str(i) for i in key_word_bigram])\n",
    "df_all_key_word_bigram = pd.DataFrame(data={'bigrams': all_key_word_bigram.split('/')})\n",
    "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('buena', 'universidad')\"]\n",
    "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('universidad', 'buena')\"]\n",
    "count_bigram_tweets = df_all_key_word_bigram.value_counts().reset_index()\n",
    "count_bigram_tweets = count_bigram_tweets[count_bigram_tweets[\"bigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
    "count_bigram_tweets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigrams</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('iberoamericana', 'educacion', 'distancia')</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('ried', 'revista', 'iberoamericana')</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('revista', 'iberoamericana', 'educacion')</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('rt', 'antilopino', '‚Äú')</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('universidad', 'central', 'venezuela')</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('tendencias', 'pais', 'retina')</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('mit', 'technology', 'review')</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('technology', 'review', 'espa√±ol')</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('america', 'latina', 'caribe')</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('repositorio', 'institucional', 'universidad')</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('the', 'new', 'york')</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('rt', 'gorkaorive', 'nuevo')</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('institucional', 'universidad', 'central')</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('new', 'york', 'times')</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('revista', 'facultad', 'medicina')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('academia', 'nacional', 'medicina')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('rt', 'fuedicho', '‚Äú')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('rt', 'gorkaorive', 'estudio')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('pedagogica', 'docencia', 'universitaria')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('rt', 'doctormacias', 'pandemia')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           trigrams   0\n",
       "0      ('iberoamericana', 'educacion', 'distancia')  24\n",
       "1             ('ried', 'revista', 'iberoamericana')  23\n",
       "2        ('revista', 'iberoamericana', 'educacion')  23\n",
       "3                         ('rt', 'antilopino', '‚Äú')  20\n",
       "4           ('universidad', 'central', 'venezuela')  13\n",
       "5                  ('tendencias', 'pais', 'retina')   9\n",
       "6                   ('mit', 'technology', 'review')   8\n",
       "7               ('technology', 'review', 'espa√±ol')   8\n",
       "8                   ('america', 'latina', 'caribe')   7\n",
       "9   ('repositorio', 'institucional', 'universidad')   7\n",
       "10                           ('the', 'new', 'york')   6\n",
       "11                    ('rt', 'gorkaorive', 'nuevo')   6\n",
       "12      ('institucional', 'universidad', 'central')   6\n",
       "13                         ('new', 'york', 'times')   6\n",
       "14              ('revista', 'facultad', 'medicina')   5\n",
       "15             ('academia', 'nacional', 'medicina')   5\n",
       "16                          ('rt', 'fuedicho', '‚Äú')   5\n",
       "17                  ('rt', 'gorkaorive', 'estudio')   4\n",
       "18      ('pedagogica', 'docencia', 'universitaria')   4\n",
       "19               ('rt', 'doctormacias', 'pandemia')   4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_word_trigram = o_df[o_df['clean_text']!='']['trigram_key_words'].to_list()\n",
    "all_key_word_trigram = '/'.join([str(i) for i in key_word_trigram])\n",
    "df_all_key_word_trigram = pd.DataFrame(data={'trigrams': all_key_word_trigram.split('/')})\n",
    "\n",
    "count_trigram_tweets = df_all_key_word_trigram.value_counts().reset_index()\n",
    "count_trigram_tweets = count_trigram_tweets[count_trigram_tweets[\"trigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
    "count_trigram_tweets[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A√±adir visualizaci√≥n en el mismo notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>bigram_key_words</th>\n",
       "      <th>trigram_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>1521302809450369026</td>\n",
       "      <td>Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>guia evaluacion conservacion datos investigaci...</td>\n",
       "      <td>('guia', 'evaluacion')/('evaluacion', 'conserv...</td>\n",
       "      <td>('guia', 'evaluacion', 'conservacion')/('evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1519344752981909504</td>\n",
       "      <td>RT @gorka_orive: Os recomiendo seguir la infor...</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>rt gorkaorive recomiendo seguir informacion va...</td>\n",
       "      <td>('rt', 'gorkaorive')/('gorkaorive', 'recomiend...</td>\n",
       "      <td>('rt', 'gorkaorive', 'recomiendo')/('gorkaoriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1519344657590800391</td>\n",
       "      <td>RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>rt drpontecarlosi nuevas guias uso aspirina pr...</td>\n",
       "      <td>('rt', 'drpontecarlosi')/('drpontecarlosi', 'n...</td>\n",
       "      <td>('rt', 'drpontecarlosi', 'nuevas')/('drponteca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_at                   id  \\\n",
       "0  2022-05-03  1521302809450369026   \n",
       "1  2022-04-27  1519344752981909504   \n",
       "2  2022-04-27  1519344657590800391   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  Gu√≠a de evaluaci√≥n para la conservaci√≥n de dat...              2   \n",
       "1  RT @gorka_orive: Os recomiendo seguir la infor...            115   \n",
       "2  RT @DrPontecarlosi: Nuevas gu√≠as sobre uso de ...             74   \n",
       "\n",
       "   like_count                                         clean_text  \\\n",
       "0           1  guia evaluacion conservacion datos investigaci...   \n",
       "1           0  rt gorkaorive recomiendo seguir informacion va...   \n",
       "2           0  rt drpontecarlosi nuevas guias uso aspirina pr...   \n",
       "\n",
       "                                    bigram_key_words  \\\n",
       "0  ('guia', 'evaluacion')/('evaluacion', 'conserv...   \n",
       "1  ('rt', 'gorkaorive')/('gorkaorive', 'recomiend...   \n",
       "2  ('rt', 'drpontecarlosi')/('drpontecarlosi', 'n...   \n",
       "\n",
       "                                   trigram_key_words  \n",
       "0  ('guia', 'evaluacion', 'conservacion')/('evalu...  \n",
       "1  ('rt', 'gorkaorive', 'recomiendo')/('gorkaoriv...  \n",
       "2  ('rt', 'drpontecarlosi', 'nuevas')/('drponteca...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ea8b4",
   "metadata": {},
   "source": [
    "# EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "088bfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Ngram_Analysis.xlsx') as writer:  \n",
    "    o_df.to_excel(writer, sheet_name='Base')    \n",
    "    count_trigram_tweets.to_excel(writer, sheet_name='Bigrams')\n",
    "    count_bigram_tweets.to_excel(writer, sheet_name='Trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ac031",
   "metadata": {},
   "source": [
    "## NLP categorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c6c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea27ed6a",
   "metadata": {},
   "source": [
    "## Oportunidades de Mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b617633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_df_nouns=o_df[[\"clean_text\"]].dropna().reset_index(inplace=False).dropna().drop('index', axis=1, inplace=False)\n",
    "\n",
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common Spanish stop words\n",
    "\n",
    "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
    "lines_df.columns = [\"spanish_stopwords\"]\n",
    "lines=list(lines_df[\"spanish_stopwords\"])\n",
    "#lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b18067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicaremos varios rounds de limpieza\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?¬ø\\]\\%', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Segundo round\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶¬´¬ª]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "'''\n",
    "# Como no tenemos un Lemmatizer en espa√±ol, hacemos manualmente algunas conversiones\n",
    "# OJO: esto realmente no se hace a mano!!!\n",
    "\n",
    "def detectadas(palabra):\n",
    "    eliminar_s = ('libreros','textos','papelitos','monedas','p√°ginas','an√©cdotas','perros','cuadernos','blogs',\n",
    "                  'revistas','caballos','vecinos','madres','puntos','ricos','libros')\n",
    "    if palabra in eliminar_s :\n",
    "        return palabra[:-1]\n",
    "    eliminar_es = ('mundiales','lectores','campeones','man√≠es','ustedes','autores')\n",
    "    if palabra in eliminar_es:\n",
    "        return palabra[:-2]\n",
    "    return palabra\n",
    "\n",
    "def clean_text_round3(text):\n",
    "    return \" \".join([detectadas(word) for word in text.split()])\n",
    "    \n",
    "'''\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "#round3 = lambda x: clean_text_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37b74ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos la primer limpieza\n",
    "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round1))\n",
    "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round2))\n",
    "#data_clean = pd.DataFrame(data_nouns[\"OportunidadesMejoraTextClean\"].apply(round3))\n",
    "\n",
    "# Esto es un nuevo campo por si quisieramos agregar alguna info adicional a cada a√±o\n",
    "# Nuestro caso repetimos los a√±os, nos servir√° para alguna visualizaci√≥n\n",
    "full_names = list(data_clean.reset_index()[\"index\"])#['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "o_df_nouns['full_name'] = full_names# nombre de indices del comentario\n",
    "\n",
    "# Lo guardamos como pickle\n",
    "o_df_nouns.to_pickle(\"data_nouns.pkl\")\n",
    "\n",
    "cv = CountVectorizer(stop_words=lines)\n",
    "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "# Lo guardamos como pickle\n",
    "data_dtm.to_pickle(\"dtm.pkl\")\n",
    "\n",
    "# Lo guardamos como pickle tambi√©n\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc4db607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>abacantvreportes</th>\n",
       "      <th>abaratarlos</th>\n",
       "      <th>abarca</th>\n",
       "      <th>abbottglobal</th>\n",
       "      <th>abcedario</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abiert</th>\n",
       "      <th>abierta</th>\n",
       "      <th>abiertas</th>\n",
       "      <th>...</th>\n",
       "      <th>zelanda</th>\n",
       "      <th>zenecaoxford</th>\n",
       "      <th>zonas</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoonosis</th>\n",
       "      <th>zostavax</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zotero</th>\n",
       "      <th>zulmacucunuba</th>\n",
       "      <th>zuluaga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2344 rows √ó 6908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aba  abacantvreportes  abaratarlos  abarca  abbottglobal  abcedario  \\\n",
       "0       0                 0            0       0             0          0   \n",
       "1       0                 0            0       0             0          0   \n",
       "2       0                 0            0       0             0          0   \n",
       "3       0                 0            0       0             0          0   \n",
       "4       0                 0            0       0             0          0   \n",
       "...   ...               ...          ...     ...           ...        ...   \n",
       "2339    0                 0            0       0             1          0   \n",
       "2340    0                 0            0       0             0          0   \n",
       "2341    0                 0            0       0             0          0   \n",
       "2342    0                 0            0       0             0          0   \n",
       "2343    0                 0            0       0             0          0   \n",
       "\n",
       "      abdomen  abiert  abierta  abiertas  ...  zelanda  zenecaoxford  zonas  \\\n",
       "0           0       0        0         0  ...        0             0      0   \n",
       "1           0       0        0         0  ...        0             0      0   \n",
       "2           0       0        0         0  ...        0             0      0   \n",
       "3           0       0        0         0  ...        0             0      0   \n",
       "4           0       0        0         0  ...        0             0      0   \n",
       "...       ...     ...      ...       ...  ...      ...           ...    ...   \n",
       "2339        0       0        0         0  ...        0             0      0   \n",
       "2340        0       0        0         0  ...        0             0      0   \n",
       "2341        0       0        0         0  ...        0             0      0   \n",
       "2342        0       0        0         0  ...        0             0      0   \n",
       "2343        0       0        0         0  ...        0             0      0   \n",
       "\n",
       "      zoom  zoonosis  zostavax  zoster  zotero  zulmacucunuba  zuluaga  \n",
       "0        0         0         0       0       0              0        0  \n",
       "1        0         0         0       0       0              0        0  \n",
       "2        0         0         0       0       0              0        0  \n",
       "3        0         0         0       0       0              0        0  \n",
       "4        0         0         0       0       0              0        0  \n",
       "...    ...       ...       ...     ...     ...            ...      ...  \n",
       "2339     0         0         0       0       0              0        0  \n",
       "2340     0         0         0       0       0              0        0  \n",
       "2341     0         0         0       0       0              0        0  \n",
       "2342     0         0         0       0       0              0        0  \n",
       "2343     0         0         0       0       0              0        0  \n",
       "\n",
       "[2344 rows x 6908 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
    "lines_df.columns = [\"spanish_stopwords\"]\n",
    "stop_words=list(lines_df[\"spanish_stopwords\"])\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")\n",
    "# Let's read in our document-term matrix\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc3ac909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "#tdm.tail()\n",
    "\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cebedc",
   "metadata": {},
   "source": [
    "## An√°lisis de tem√°ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dfc590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"rt\" + 0.009*\"educacion\" + 0.006*\"aprendizaje\" + 0.005*\"salud\" + 0.004*\"investigacion\" + 0.004*\"pais\" + 0.003*\"educativa\" + 0.003*\"futuro\" + 0.003*\"tendencias\" + 0.003*\"ciencia\"'),\n",
       " (1,\n",
       "  '0.032*\"rt\" + 0.007*\"digital\" + 0.004*\"mundo\" + 0.003*\"vida\" + 0.003*\"coronavirus\" + 0.003*\"educacion\" + 0.003*\"enfermedad\" + 0.003*\"personas\" + 0.003*\"importante\" + 0.003*\"dia\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2643fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['rt', 'educacion', 'aprendizaje', 'salud', 'investigacion', 'pais', 'educativa', 'futuro', 'tendencias', 'ciencia']\n",
      "Topic: 1 \n",
      "Words: ['rt', 'digital', 'mundo', 'vida', 'coronavirus', 'educacion', 'enfermedad', 'personas', 'importante', 'dia']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "777efef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"rt\" + 0.014*\"educacion\" + 0.007*\"digital\" + 0.005*\"tecnologia\" + 0.005*\"investigacion\" + 0.004*\"salud\" + 0.004*\"mejores\" + 0.004*\"revistas\" + 0.004*\"distancia\" + 0.004*\"datos\"'),\n",
       " (1,\n",
       "  '0.030*\"rt\" + 0.004*\"dia\" + 0.004*\"google\" + 0.004*\"puede\" + 0.004*\"vida\" + 0.004*\"millones\" + 0.003*\"pais\" + 0.003*\"rigotordoc\" + 0.003*\"personas\" + 0.003*\"hacer\"'),\n",
       " (2,\n",
       "  '0.020*\"rt\" + 0.006*\"aprendizaje\" + 0.005*\"coronavirus\" + 0.005*\"venezuela\" + 0.005*\"educacion\" + 0.004*\"ciencia\" + 0.004*\"acceso\" + 0.004*\"ideas\" + 0.003*\"saber\" + 0.003*\"futuro\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda_3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29835161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"rt\" + 0.007*\"educacion\" + 0.006*\"venezuela\" + 0.004*\"enfermedad\" + 0.004*\"puede\" + 0.004*\"ciencia\" + 0.004*\"quiere\" + 0.004*\"digital\" + 0.004*\"salud\" + 0.004*\"mismo\"'),\n",
       " (1,\n",
       "  '0.025*\"rt\" + 0.007*\"tecnologia\" + 0.005*\"digital\" + 0.005*\"educacion\" + 0.004*\"mundo\" + 0.004*\"asi\" + 0.004*\"aula\" + 0.004*\"salud\" + 0.003*\"menos\" + 0.003*\"ve\"'),\n",
       " (2,\n",
       "  '0.009*\"rt\" + 0.006*\"aprendizaje\" + 0.006*\"educacion\" + 0.005*\"digital\" + 0.005*\"via\" + 0.005*\"herramientas\" + 0.005*\"universidad\" + 0.005*\"investigacion\" + 0.004*\"futuro\" + 0.004*\"educativa\"'),\n",
       " (3,\n",
       "  '0.021*\"rt\" + 0.008*\"educacion\" + 0.008*\"pais\" + 0.005*\"articulos\" + 0.005*\"importante\" + 0.004*\"retina\" + 0.004*\"salud\" + 0.004*\"personas\" + 0.004*\"uso\" + 0.004*\"rigotordoc\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda_4 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f1a5940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.037*\"educacion\" + 0.016*\"distancia\" + 0.010*\"revista\" + 0.009*\"digitales\" + 0.009*\"ried\" + 0.009*\"rt\" + 0.007*\"pais\" + 0.007*\"digital\" + 0.007*\"puede\" + 0.006*\"mundo\"'),\n",
       " (1,\n",
       "  '0.011*\"educativa\" + 0.010*\"rt\" + 0.009*\"digital\" + 0.008*\"innovacion\" + 0.007*\"big\" + 0.007*\"crear\" + 0.007*\"vida\" + 0.007*\"formacion\" + 0.007*\"dia\" + 0.006*\"castigo\"'),\n",
       " (2,\n",
       "  '0.024*\"salud\" + 0.022*\"rt\" + 0.013*\"ciencia\" + 0.010*\"articulos\" + 0.008*\"abierta\" + 0.007*\"nunca\" + 0.007*\"mundial\" + 0.007*\"quiere\" + 0.006*\"datos\" + 0.006*\"excelente\"'),\n",
       " (3,\n",
       "  '0.027*\"rt\" + 0.007*\"herramientas\" + 0.006*\"hacer\" + 0.006*\"estudios\" + 0.006*\"tic\" + 0.006*\"uso\" + 0.006*\"educativos\" + 0.006*\"respuesta\" + 0.005*\"asi\" + 0.005*\"academicos\"'),\n",
       " (4,\n",
       "  '0.012*\"derecho\" + 0.008*\"necesario\" + 0.008*\"ejemplo\" + 0.008*\"buscan\" + 0.008*\"convierte\" + 0.007*\"todas\" + 0.007*\"mismo\" + 0.007*\"investigacion\" + 0.006*\"moviles\" + 0.006*\"experiencia\"'),\n",
       " (5,\n",
       "  '0.022*\"rt\" + 0.015*\"mejores\" + 0.013*\"futuro\" + 0.011*\"ideas\" + 0.010*\"aprendizaje\" + 0.009*\"inteligencia\" + 0.009*\"numero\" + 0.008*\"herramientas\" + 0.008*\"artificial\" + 0.007*\"menos\"'),\n",
       " (6,\n",
       "  '0.018*\"rt\" + 0.009*\"aprende\" + 0.007*\"america\" + 0.006*\"impacto\" + 0.006*\"oportunidad\" + 0.006*\"latina\" + 0.006*\"casa\" + 0.006*\"youtube\" + 0.006*\"posible\" + 0.006*\"funciona\"'),\n",
       " (7,\n",
       "  '0.025*\"rt\" + 0.013*\"via\" + 0.009*\"pais\" + 0.008*\"digital\" + 0.008*\"educacion\" + 0.008*\"aprender\" + 0.007*\"enfermedad\" + 0.007*\"tendencias\" + 0.007*\"clave\" + 0.007*\"retina\"'),\n",
       " (8,\n",
       "  '0.043*\"rt\" + 0.008*\"revistas\" + 0.008*\"venezuela\" + 0.007*\"ucv\" + 0.007*\"a√±os\" + 0.007*\"idiucv\" + 0.006*\"saberucv\" + 0.006*\"universidad\" + 0.005*\"informacion\" + 0.005*\"siglo\"'),\n",
       " (9,\n",
       "  '0.017*\"rt\" + 0.015*\"google\" + 0.008*\"usar\" + 0.008*\"educacion\" + 0.008*\"atencion\" + 0.006*\"tambien\" + 0.006*\"primaria\" + 0.006*\"mejor\" + 0.006*\"decir\" + 0.006*\"acceso\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 10\n",
    "lda_10 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda_10.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b836d",
   "metadata": {},
   "source": [
    "### Ordenamiento por tem√°ticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d9aed68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['educacion', 'distancia', 'revista', 'digitales', 'ried', 'rt']\n",
      "Topic: 1 \n",
      "Words: ['educativa', 'rt', 'digital', 'innovacion', 'big', 'crear']\n",
      "Topic: 2 \n",
      "Words: ['salud', 'rt', 'ciencia', 'articulos', 'abierta', 'nunca']\n",
      "Topic: 3 \n",
      "Words: ['rt', 'herramientas', 'hacer', 'estudios', 'tic', 'uso']\n",
      "Topic: 4 \n",
      "Words: ['derecho', 'necesario', 'ejemplo', 'buscan', 'convierte', 'todas']\n",
      "Topic: 5 \n",
      "Words: ['rt', 'mejores', 'futuro', 'ideas', 'aprendizaje', 'inteligencia']\n",
      "Topic: 6 \n",
      "Words: ['rt', 'aprende', 'america', 'impacto', 'oportunidad', 'latina']\n",
      "Topic: 7 \n",
      "Words: ['rt', 'via', 'pais', 'digital', 'educacion', 'aprender']\n",
      "Topic: 8 \n",
      "Words: ['rt', 'revistas', 'venezuela', 'ucv', 'a√±os', 'idiucv']\n",
      "Topic: 9 \n",
      "Words: ['rt', 'google', 'usar', 'educacion', 'atencion', 'tambien']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_10.show_topics(formatted=False, num_words= 6):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hackathon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
